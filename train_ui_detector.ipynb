{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA UI Element Object Detection Training\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ GAIA í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œì—ì„œ ìˆ˜ì§‘í•œ UI ìš”ì†Œ ë°ì´í„°ë¡œ YOLO ê°ì²´ ì¸ì‹ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì‹¤í–‰ í™˜ê²½:**\n",
    "- Google Colab (ë¬´ë£Œ T4 GPU)\n",
    "- ë˜ëŠ” ë¡œì»¬ ë§¥ë¶ (Apple Silicon MPS)\n",
    "\n",
    "**ì˜ˆìƒ ì‹œê°„:**\n",
    "- Colab T4: 1-2ì‹œê°„\n",
    "- M1/M2 ë§¥ë¶: 1-2ì‹œê°„\n",
    "- M3 ë§¥ë¶: 30ë¶„-1ì‹œê°„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install ultralytics opencv-python pillow\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ í™•ì¸\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon\n",
    "    print(\"âœ… Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"0\"  # NVIDIA GPU\n",
    "    print(f\"âœ… Using NVIDIA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"âš ï¸ Using CPU (will be slow)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ë°ì´í„°ì…‹ ì—…ë¡œë“œ (Colabë§Œ)\n",
    "\n",
    "**ë¡œì»¬ ë§¥ë¶ì—ì„œ ì‹¤í–‰ ì‹œ ì´ ì…€ì€ ìŠ¤í‚µí•˜ì„¸ìš”!**\n",
    "\n",
    "Colabì—ì„œ ì‹¤í–‰ ì‹œ:\n",
    "1. `artifacts/training_data` í´ë”ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•\n",
    "2. Google Driveì— ì—…ë¡œë“œ\n",
    "3. ì•„ë˜ ì½”ë“œë¡œ ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabì—ì„œë§Œ ì‹¤í–‰\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # ZIP ì••ì¶• í•´ì œ (Google Driveì— training_data.zip ì—…ë¡œë“œí•œ ê²½ìš°)\n",
    "    !unzip -q /content/drive/MyDrive/training_data.zip -d /content/\n",
    "    \n",
    "    DATA_DIR = \"/content/training_data\"\n",
    "    print(f\"âœ… Data loaded from Google Drive\")\n",
    "except:\n",
    "    # ë¡œì»¬ ì‹¤í–‰\n",
    "    DATA_DIR = \"artifacts/training_data\"\n",
    "    print(f\"âœ… Using local data: {DATA_DIR}\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ í™•ì¸\n",
    "images_dir = Path(DATA_DIR) / \"images\"\n",
    "labels_dir = Path(DATA_DIR) / \"labels\"\n",
    "\n",
    "num_images = len(list(images_dir.glob(\"*.png\")))\n",
    "num_labels = len(list(labels_dir.glob(\"*.txt\")))\n",
    "\n",
    "print(f\"ğŸ“Š Dataset statistics:\")\n",
    "print(f\"   Images: {num_images}\")\n",
    "print(f\"   Labels: {num_labels}\")\n",
    "\n",
    "if num_images < 50:\n",
    "    print(\"âš ï¸ WARNING: Less than 50 images! Collect more data for better results.\")\n",
    "elif num_images < 200:\n",
    "    print(\"âš ï¸ 50-200 images: Model will work but accuracy may be limited.\")\n",
    "else:\n",
    "    print(\"âœ… Good dataset size! Training should produce good results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ë°ì´í„°ì…‹ ë¶„í•  (Train/Val)\n",
    "\n",
    "80% í•™ìŠµ, 20% ê²€ì¦ìœ¼ë¡œ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ í´ë” ìƒì„±\n",
    "train_images = Path(DATA_DIR) / \"images\" / \"train\"\n",
    "val_images = Path(DATA_DIR) / \"images\" / \"val\"\n",
    "train_labels = Path(DATA_DIR) / \"labels\" / \"train\"\n",
    "val_labels = Path(DATA_DIR) / \"labels\" / \"val\"\n",
    "\n",
    "for folder in [train_images, val_images, train_labels, val_labels]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼\n",
    "all_images = list(images_dir.glob(\"*.png\"))\n",
    "random.shuffle(all_images)\n",
    "\n",
    "# 80/20 ë¶„í• \n",
    "split_idx = int(len(all_images) * 0.8)\n",
    "train_imgs = all_images[:split_idx]\n",
    "val_imgs = all_images[split_idx:]\n",
    "\n",
    "# íŒŒì¼ ë³µì‚¬\n",
    "for img_path in train_imgs:\n",
    "    label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "    shutil.copy(img_path, train_images / img_path.name)\n",
    "    if label_path.exists():\n",
    "        shutil.copy(label_path, train_labels / label_path.name)\n",
    "\n",
    "for img_path in val_imgs:\n",
    "    label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "    shutil.copy(img_path, val_images / img_path.name)\n",
    "    if label_path.exists():\n",
    "        shutil.copy(label_path, val_labels / label_path.name)\n",
    "\n",
    "print(f\"âœ… Dataset split:\")\n",
    "print(f\"   Train: {len(train_imgs)} images\")\n",
    "print(f\"   Val: {len(val_imgs)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: data.yaml ì—…ë°ì´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO data.yaml ìƒì„±\n",
    "yaml_content = f\"\"\"# GAIA UI Element Detection Dataset\n",
    "path: {Path(DATA_DIR).absolute()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: button\n",
    "  1: input\n",
    "  2: link\n",
    "  3: checkbox\n",
    "  4: radio\n",
    "  5: dropdown\n",
    "  6: text\n",
    "  7: image\n",
    "\n",
    "# Number of classes\n",
    "nc: 8\n",
    "\"\"\"\n",
    "\n",
    "yaml_path = Path(DATA_DIR) / \"data.yaml\"\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"âœ… Created {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "**YOLOv8n (nano)**: ê°€ì¥ ë¹ ë¥´ê³  ê°€ë²¼ì›€, ì‹¤ì‹œê°„ ì¶”ë¡ ì— ì í•©  \n",
    "**YOLOv8s (small)**: ì¤‘ê°„ í¬ê¸°, ë” ì •í™•í•¨  \n",
    "**YOLOv8m (medium)**: í° í¬ê¸°, ê°€ì¥ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼\n",
    "\n",
    "ë§¥ë¶ì€ nano ì¶”ì²œ, Colabì€ small/medium ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í¬ê¸° ì„ íƒ\n",
    "MODEL_SIZE = \"yolov8n.pt\"  # nano (ê°€ì¥ ë¹ ë¦„)\n",
    "# MODEL_SIZE = \"yolov8s.pt\"  # small (ì¤‘ê°„)\n",
    "# MODEL_SIZE = \"yolov8m.pt\"  # medium (ê°€ì¥ ì •í™•)\n",
    "\n",
    "# í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "EPOCHS = 50  # ë§¥ë¶: 30-50, Colab: 50-100\n",
    "BATCH_SIZE = 16  # ë§¥ë¶: 8-16, Colab: 16-32\n",
    "IMG_SIZE = 640  # ì´ë¯¸ì§€ í¬ê¸°\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = YOLO(MODEL_SIZE)\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "print(f\"ğŸš€ Training started with {MODEL_SIZE}...\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Expected time: {'1-2 hours' if device == 'mps' else '1-1.5 hours' if device == '0' else '5-10 hours'}\")\n",
    "\n",
    "results = model.train(\n",
    "    data=str(yaml_path),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    device=device,\n",
    "    \n",
    "    # ìµœì í™” ì„¤ì •\n",
    "    workers=4,  # ë°ì´í„° ë¡œë”© ì›Œì»¤\n",
    "    cache=True,  # ë©”ëª¨ë¦¬ ìºì‹± (ë¹ ë¦„)\n",
    "    amp=True,    # Mixed precision (ë¹ ë¦„)\n",
    "    \n",
    "    # ì €ì¥ ì„¤ì •\n",
    "    project='runs/detect',\n",
    "    name='ui_detector',\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # ì¡°ê¸° ì¢…ë£Œ (ê³¼ì í•© ë°©ì§€)\n",
    "    patience=10,\n",
    "    \n",
    "    # Verbose\n",
    "    verbose=True,\n",
    "    \n",
    "    # í•™ìŠµ ì¤‘ ì‹œê°í™”\n",
    "    plots=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Training completed!\")\n",
    "print(f\"   Best model: {results.save_dir}/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€\n",
    "metrics = model.val()\n",
    "\n",
    "print(f\"ğŸ“Š Validation Results:\")\n",
    "print(f\"   mAP50: {metrics.box.map50:.3f}\")\n",
    "print(f\"   mAP50-95: {metrics.box.map:.3f}\")\n",
    "print(f\"   Precision: {metrics.box.mp:.3f}\")\n",
    "print(f\"   Recall: {metrics.box.mr:.3f}\")\n",
    "\n",
    "if metrics.box.map50 > 0.8:\n",
    "    print(\"âœ… Excellent! Model is very accurate.\")\n",
    "elif metrics.box.map50 > 0.6:\n",
    "    print(\"âœ… Good! Model should work well.\")\n",
    "elif metrics.box.map50 > 0.4:\n",
    "    print(\"âš ï¸ Okay. Consider collecting more data or training longer.\")\n",
    "else:\n",
    "    print(\"âŒ Poor accuracy. Need more/better training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: í…ŒìŠ¤íŠ¸ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ì¶”ë¡ \n",
    "test_img = list(val_images.glob(\"*.png\"))[0]\n",
    "\n",
    "results = model.predict(\n",
    "    source=str(test_img),\n",
    "    conf=0.5,  # Confidence threshold\n",
    "    save=True,  # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥\n",
    "    show_labels=True,\n",
    "    show_conf=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Prediction saved to: {results[0].save_dir}\")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for box in results[0].boxes:\n",
    "    class_id = int(box.cls[0])\n",
    "    class_name = model.names[class_id]\n",
    "    confidence = float(box.conf[0])\n",
    "    x1, y1, x2, y2 = box.xyxy[0]\n",
    "    print(f\"   {class_name}: {confidence:.2f} at ({x1:.0f}, {y1:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: ëª¨ë¸ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "best_model_path = Path(results.save_dir) / \"weights\" / \"best.pt\"\n",
    "output_path = \"ui_detector.pt\"\n",
    "\n",
    "shutil.copy(best_model_path, output_path)\n",
    "print(f\"âœ… Model saved to: {output_path}\")\n",
    "\n",
    "# Colabì—ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "    print(\"ğŸ“¥ Downloading model...\")\n",
    "except:\n",
    "    print(f\"ğŸ’¾ Model ready at: {Path(output_path).absolute()}\")\n",
    "    print(f\"   Copy to: gaia/models/ui_detector.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ì†ë„ ë²¤ì¹˜ë§ˆí¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ì†ë„ í…ŒìŠ¤íŠ¸\n",
    "test_img = str(list(val_images.glob(\"*.png\"))[0])\n",
    "\n",
    "# Warm up\n",
    "for _ in range(3):\n",
    "    model.predict(test_img, verbose=False)\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "for _ in range(20):\n",
    "    start = time.time()\n",
    "    model.predict(test_img, verbose=False)\n",
    "    times.append((time.time() - start) * 1000)  # ms\n",
    "\n",
    "print(f\"âš¡ Inference Speed:\")\n",
    "print(f\"   Average: {np.mean(times):.1f}ms\")\n",
    "print(f\"   Min: {np.min(times):.1f}ms\")\n",
    "print(f\"   Max: {np.max(times):.1f}ms\")\n",
    "\n",
    "if np.mean(times) < 50:\n",
    "    print(\"âœ… Excellent! Real-time capable.\")\n",
    "elif np.mean(times) < 100:\n",
    "    print(\"âœ… Good! Fast enough for testing.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Slow. Consider using smaller model (yolov8n).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™„ë£Œ! ğŸ‰\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„:**\n",
    "1. `ui_detector.pt` íŒŒì¼ì„ `gaia/models/`ì— ë³µì‚¬\n",
    "2. `ui_detector.py` í†µí•© ì½”ë“œ ì‹¤í–‰\n",
    "3. GAIA í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ì—¬ ê°ì²´ ì¸ì‹ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "**ì˜ˆìƒ ì„±ëŠ¥:**\n",
    "- ì…€ë ‰í„° íƒìƒ‰ ì„±ê³µë¥ : 60% â†’ 85%+\n",
    "- LLM í˜¸ì¶œ íšŸìˆ˜: 100% â†’ 30%\n",
    "- ì‹¤í–‰ ì†ë„: 3ë°° ë¹ ë¦„\n",
    "- ë¹„ìš©: GPT-4V $10/1000íšŒ â†’ $0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
